{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR4qfYrVoO4v"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWVONJxCobPc"
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78ZTCIXoof2f",
    "outputId": "cf7c8f82-7aab-49ce-a68c-59b38e957cc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "#Dataset Imports\n",
    "import csv\n",
    "from IPython.display import Audio, display\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ORNHnSFroP0"
   },
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set directory to the Audios folder in MSP dataset\n",
    "AUDIO_ROOT = 'C:\\\\Users\\\\Justin\\\\Documents\\\\idl\\\\2022\\\\Project\\\\Audios_fixed\\\\Audios\\\\'\n",
    "#Set path to labels_consensus in MSP dataset\n",
    "LABELS_DIR = 'C:\\\\Users\\\\Justin\\\\Documents\\\\idl\\\\2022\\\\Project\\\\labels\\\\labels\\\\labels_concensus.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 167814])\n",
      "16000\n",
      "['MSP-PODCAST_0001_0008.wav', 'N', '2.2', '4.0', '2.6', '30', 'Male', 'Test1']\n"
     ]
    }
   ],
   "source": [
    "#Load the directory\n",
    "names = sorted(os.listdir(AUDIO_ROOT))\n",
    "data1 = AUDIO_ROOT + names[0]\n",
    "#torchaudio.load requires you to install some programs if you get 'No audio I/O backend is available' error\n",
    "#https://stackoverflow.com/questions/62543843/cannot-import-torch-audio-no-audio-backend-is-available\n",
    "waveform, sample_rate = torchaudio.load(data1)\n",
    "print(waveform.shape)\n",
    "print(sample_rate)\n",
    "\n",
    "#Load label csv file\n",
    "with open('labels_concensus.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    labels = sorted(list(reader)[1:])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotion Classifier Map (Emotion tag to int for model)\n",
    "#Angry, Sad, Happy, Surprise, Fear, Disgust, Contempt, Neutral, Other\n",
    "EMOMAP = {'A':1, 'S':2, 'H':3, 'U':4, 'F':5, 'D':6, 'C':7, 'N':8, 'O':9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "afd0_vlbJmr_"
   },
   "outputs": [],
   "source": [
    "class MSPDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    #Initialize the dataset based on the recommended split in MSP dataset.\n",
    "    def __init__(self, train = False, valid = False, test1 = False, test2 = False): \n",
    "        \n",
    "        self.audio_dir = AUDIO_ROOT\n",
    "        self.labels_dir = LABELS_DIR\n",
    "        self.audio_names = sorted(names)\n",
    "        self.labels_list = labels\n",
    "        self.EMOMAP = EMOMAP\n",
    "\n",
    "        self.audio = []\n",
    "        self.labels = []\n",
    "        \n",
    "        #What type of dataset are we making\n",
    "        setType = 'Train'\n",
    "        if valid:\n",
    "            setType = 'Validation'\n",
    "        elif test1:\n",
    "            setType = 'Test1'\n",
    "        elif test2:\n",
    "            setType = 'Test2'\n",
    "        print(setType)       \n",
    "        \n",
    "        #Sanitycheck1\n",
    "        assert(len(self.audio_names) == len(self.labels_list))\n",
    "        \n",
    "        for i in tqdm(range(0, len(self.audio_names))):\n",
    "            assert(self.audio_names[i] == self.labels_list[i][0])\n",
    "            if self.labels_list[i][7] != setType or self.labels_list[i][1] == 'X':\n",
    "                continue\n",
    "            #43 Audio files from 1904 podcast seems to be broken. Torchaudio load returns 'no data chunk'\n",
    "            if self.labels_list[i][0].startswith('MSP-PODCAST_1904'):\n",
    "                continue\n",
    "            self.audio.append(self.audio_dir + self.audio_names[i])\n",
    "            self.labels.append(self.EMOMAP[self.labels_list[i][1]])         \n",
    "        \n",
    "        self.length = len(self.audio)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        audio = self.audio[ind]\n",
    "        label = self.labels[ind]\n",
    "        #load audio when getting the item. If we do it in init, computer blue screens.\n",
    "        waveform, sample_rate = torchaudio.load(audio)\n",
    "        return waveform, label\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        batch_audio = [x[0].reshape(-1) for x in batch]\n",
    "        audio_lengths = torch.LongTensor([len(x) for x in batch_audio])\n",
    "        batch_audio = pad_sequence(batch_audio, padding_value=0.0, batch_first = True)\n",
    "        batch_label = [x[1] for x in batch]\n",
    "        \n",
    "        return batch_audio, audio_lengths, torch.tensor(batch_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmuPk9J6L8dz"
   },
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_kG0gU2x4hH",
    "outputId": "95a65754-500e-42ba-99c8-7b90bd6e1ff4",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get me RAMMM!!!! \n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 73042/73042 [00:00<00:00, 1162494.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 73042/73042 [00:00<00:00, 2092453.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 73042/73042 [00:00<00:00, 1786326.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36011\n",
      "6346\n",
      "12371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset objects.\n",
    "train_data = MSPDataset(train = True) \n",
    "val_data = MSPDataset(valid = True) \n",
    "test_data = MSPDataset(test1 = True)\n",
    "\n",
    "print(train_data.__len__())\n",
    "print(val_data.__len__())\n",
    "print(test_data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mzoYfTKu14s",
    "outputId": "2197e432-f0c3-419a-eb88-0cb0d3af6e43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  100\n",
      "Train dataset samples = 36011, batches = 361\n",
      "Val dataset samples = 6346, batches = 64\n",
      "Test dataset samples = 12371, batches = 124\n"
     ]
    }
   ],
   "source": [
    "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, num_workers= 0,\n",
    "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
    "                                           shuffle= True, collate_fn = train_data.collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, num_workers= 0,\n",
    "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
    "                                           shuffle= True,collate_fn=val_data.collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, num_workers= 0,\n",
    "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
    "                                           shuffle= False, collate_fn=test_data.collate_fn)\n",
    "\n",
    "print(\"Batch size: \", BATCH_SIZE)\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXMtwyviKaxK",
    "outputId": "8ae0460a-7492-422c-c1d1-2ea5bab2c0a4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 326081])\n",
      "tensor([ 95041, 158720,  47104, 110592,  78068, 156161,  96256, 326081, 112321,\n",
      "        130721,  84321,  65281, 140480,  94208, 115681,  48161, 138401,  45056,\n",
      "         65536, 174080,  65121, 173921, 157696, 118784,  46722, 117761,  66241,\n",
      "        121440, 102881,  59841,  57344,  90112,  69601,  78241,  47361,  81121,\n",
      "         49152, 119234,  57344,  52764,  48961,  65281, 172481,  97761,  72321,\n",
      "         59361, 114257,  57601, 136640, 158080, 121281, 135361,  38912,  45440,\n",
      "         68650, 121921,  81920,  63519, 115477,  76960,  47201, 129024,  53600,\n",
      "         49152,  34816, 165888,  53281,  67681, 108161,  49152, 173282,  40960,\n",
      "         88321,  67681,  67584, 164480,  73728,  79361,  79681,  55681,  46561,\n",
      "         68161,  88064,  96481,  56321,  95041,  69632, 139361, 163840,  89601,\n",
      "         63034, 111041, 133743,  47414,  74562, 122081,  59392,  98720, 116160,\n",
      "        115681])\n",
      "tensor([4, 7, 3, 1, 8, 2, 9, 3, 3, 3, 8, 8, 8, 3, 7, 8, 7, 8, 3, 4, 3, 3, 3, 8,\n",
      "        7, 3, 8, 3, 2, 3, 7, 3, 8, 3, 8, 2, 8, 3, 8, 8, 3, 3, 8, 1, 8, 8, 3, 8,\n",
      "        3, 3, 3, 8, 8, 9, 8, 8, 6, 8, 1, 8, 6, 2, 1, 4, 8, 3, 8, 8, 8, 8, 3, 5,\n",
      "        8, 8, 8, 8, 8, 8, 8, 8, 3, 8, 8, 8, 1, 3, 4, 9, 3, 8, 8, 2, 3, 4, 7, 8,\n",
      "        3, 1, 3, 8])\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Each loader contains batched_audio which is padded to the longest sequence in the batch.\n",
    "# Lengths of each sequence before it's padded\n",
    "# Labels of the batches contained in a list.\n",
    "for i, (batch_audio, audio_lengths, batch_label) in enumerate(train_loader):\n",
    "    print(batch_audio.shape)\n",
    "    print(audio_lengths)\n",
    "    print(batch_label)\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UR4qfYrVoO4v",
    "gg3-yJ8tok34",
    "R9v5ewZDMpYA",
    "Ly4mjUUUuJhy",
    "HLad4pChcuvX",
    "tUThsowyQdN7",
    "IBwunYpyugFg",
    "kH0RAbCaMl9a",
    "qpYExu4vT4_g",
    "MY69hgxUXhTI",
    "M2H4EEj-sD32"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "50a168d2e944e50d64023c982abdec6febc400ad89db390dba97250ae813c14d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
