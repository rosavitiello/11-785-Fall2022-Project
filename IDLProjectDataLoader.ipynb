{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR4qfYrVoO4v"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWVONJxCobPc"
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78ZTCIXoof2f",
    "outputId": "cf7c8f82-7aab-49ce-a68c-59b38e957cc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Justin\\miniconda3\\envs\\MML\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "#Dataset Imports\n",
    "import csv\n",
    "from IPython.display import Audio, display\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ORNHnSFroP0"
   },
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set directory to the Audios folder in MSP dataset\n",
    "AUDIO_ROOT = 'C:\\\\Users\\\\Justin\\\\Documents\\\\idl\\\\2022\\\\Project\\\\Audios_fixed\\\\Audios\\\\'\n",
    "#Set path to labels_consensus in MSP dataset\n",
    "LABELS_DIR = 'C:\\\\Users\\\\Justin\\\\Documents\\\\idl\\\\2022\\\\Project\\\\labels\\\\labels\\\\labels_concensus.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 167814])\n",
      "16000\n",
      "['MSP-PODCAST_0001_0008.wav', 'N', '2.2', '4.0', '2.6', '30', 'Male', 'Test1']\n"
     ]
    }
   ],
   "source": [
    "#Load the directory\n",
    "names = sorted(os.listdir(AUDIO_ROOT))\n",
    "data1 = AUDIO_ROOT + names[0]\n",
    "#torchaudio.load requires you to install some programs if you get 'No audio I/O backend is available' error\n",
    "#https://stackoverflow.com/questions/62543843/cannot-import-torch-audio-no-audio-backend-is-available\n",
    "waveform, sample_rate = torchaudio.load(data1)\n",
    "print(waveform.shape)\n",
    "print(sample_rate)\n",
    "\n",
    "#Load label csv file\n",
    "with open('labels_concensus.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    labels = sorted(list(reader)[1:])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotion Classifier Map (Emotion tag to int for model)\n",
    "#Angry, Sad, Happy, Surprise, Fear, Disgust, Contempt, Neutral, Other\n",
    "EMOMAP = {'A':1, 'S':2, 'H':3, 'U':4, 'F':5, 'D':6, 'C':7, 'N':8, 'O':9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "afd0_vlbJmr_"
   },
   "outputs": [],
   "source": [
    "class MSPDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    #Initialize the dataset based on the recommended split in MSP dataset.\n",
    "    def __init__(self, train = False, valid = False, test1 = False, test2 = False): \n",
    "        \n",
    "        self.audio_dir = AUDIO_ROOT\n",
    "        self.labels_dir = LABELS_DIR\n",
    "        self.audio_names = sorted(names)\n",
    "        self.labels_list = labels\n",
    "        self.EMOMAP = EMOMAP\n",
    "\n",
    "        self.audio = []\n",
    "        self.labels = []\n",
    "        \n",
    "        #What type of dataset are we making\n",
    "        setType = 'Train'\n",
    "        if valid:\n",
    "            setType = 'Validation'\n",
    "        elif test1:\n",
    "            setType = 'Test1'\n",
    "        elif test2:\n",
    "            setType = 'Test2'\n",
    "        print(setType)       \n",
    "        \n",
    "        #Sanitycheck1\n",
    "        assert(len(self.audio_names) == len(self.labels_list))\n",
    "        \n",
    "        for i in tqdm(range(0, len(self.audio_names))):\n",
    "            assert(self.audio_names[i] == self.labels_list[i][0])\n",
    "            if self.labels_list[i][7] != setType or self.labels_list[i][1] == 'X':\n",
    "                continue\n",
    "            #43 Audio files from 1904 podcast seems to be broken. Torchaudio load returns 'no data chunk'\n",
    "            if self.labels_list[i][0].startswith('MSP-PODCAST_1904'):\n",
    "                continue\n",
    "            self.audio.append(self.audio_dir + self.audio_names[i])\n",
    "            self.labels.append(self.EMOMAP[self.labels_list[i][1]])         \n",
    "        \n",
    "        self.length = len(self.audio)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        audio = self.audio[ind]\n",
    "        label = self.labels[ind]\n",
    "        #load audio when getting the item. If we do it in init, computer blue screens.\n",
    "        waveform, sample_rate = torchaudio.load(audio)\n",
    "        return waveform, label\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        batch_audio = [x[0].reshape(-1) for x in batch]\n",
    "        audio_lengths = torch.LongTensor([len(x) for x in batch_audio])\n",
    "        batch_audio = pad_sequence(batch_audio, padding_value=0.0, batch_first = True)\n",
    "        batch_label = [x[1] for x in batch]\n",
    "        \n",
    "        return batch_audio, audio_lengths, torch.tensor(batch_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmuPk9J6L8dz"
   },
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_kG0gU2x4hH",
    "outputId": "95a65754-500e-42ba-99c8-7b90bd6e1ff4",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "482"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get me RAMMM!!!! \n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 73042/73042 [00:00<00:00, 1162494.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 73042/73042 [00:00<00:00, 2092453.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 73042/73042 [00:00<00:00, 1786326.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36011\n",
      "6346\n",
      "12371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset objects.\n",
    "train_data = MSPDataset(train = True) \n",
    "val_data = MSPDataset(valid = True) \n",
    "test_data = MSPDataset(test1 = True)\n",
    "\n",
    "print(train_data.__len__())\n",
    "print(val_data.__len__())\n",
    "print(test_data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set directory to the labelled_emotion folder in NSC dataset\n",
    "NSC_Root = 'C:\\\\Users\\\\Justin\\\\Documents\\\\idl\\\\2022\\\\Project\\\\NSC_part5_labelled_emotion\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSCDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self): \n",
    "        \n",
    "        self.audio_dir = NSC_Root\n",
    "        #quick way of looping subdirectories. Dataset only has 4 categories. \n",
    "        self.subdirectory = [('Anger\\\\', 0), ('Sad\\\\', 1), ('Happy\\\\', 2), ('Neutral\\\\', 4)]\n",
    "        self.audio = []\n",
    "        self.labels = []\n",
    "        for sub, label in subdirectory:\n",
    "            NSCaudios = os.listdir(NSC_Root + sub)\n",
    "            self.audio += [NSC_Root + sub + x for x in NSCaudios]\n",
    "            self.labels += [label]*len(NSCaudios) \n",
    "        #Sanitycheck1\n",
    "        assert(len(self.audio) == len(self.labels))\n",
    "        self.length = len(self.audio)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        audio = self.audio[ind]\n",
    "        label = self.labels[ind]\n",
    "        #load audio when getting the item. If we do it in init, computer blue screens.\n",
    "        waveform, sample_rate = torchaudio.load(audio)\n",
    "        waveform = processor(waveform, sampling_rate = 16000,padding=True, device = device)\n",
    "        # waveform = waveform.to(device)\n",
    "        # label = label.to(device)\n",
    "        waveform['labels'] = label\n",
    "\n",
    "        return waveform\n",
    "        #return waveform, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2691"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get me RAMMM!!!! \n",
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works with hugging face dataset.\n",
    "NSCTest = NSCDataset()\n",
    "NSCTest[0]\n",
    "NSCtest_dataset = Dataset.from_list(NSCTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stratified sort to train/test splits. Requires encoding the columns to classes first.\n",
    "NSCtest_dataset2 = NSCtest_dataset.class_encode_column('labels')\n",
    "NSCtest_dataset2.train_test_split(test_size = 0.1, stratify_by_column = 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSC_ASR_Root = 'C:\\\\Users\\\\Justin\\\\Documents\\\\idl\\\\2022\\\\Project\\\\top20k.tar\\\\top20k\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "model_name_or_path = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(target_sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSCSpeechDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self): \n",
    "        \n",
    "        self.audio_dir = NSC_ASR_Root + 'top20k\\\\'\n",
    "        self.labels_dir = NSC_ASR_Root + 'text.txt'\n",
    "        self.audio = []\n",
    "        self.labels = []\n",
    "        self.audio = sorted(os.listdir(self.audio_dir))\n",
    "\n",
    "        #Get the correct labels for the 20000 that we have.\n",
    "        with open(self.labels_dir) as f:\n",
    "            lines = f.readlines()\n",
    "            start = False\n",
    "            for l in lines:\n",
    "                idx = int(l[4:8])\n",
    "                #Start at APP_4001 and take 20000 from there\n",
    "                if idx >= 4001 and len(self.labels) != len(self.audio):\n",
    "                    #Remove new line and extract transcript\n",
    "                    self.labels.append(l[:-1].split(\" \", 1))\n",
    "        assert(len(self.audio) == len(self.labels))\n",
    "        self.length = len(self.audio)\n",
    "        #Sanity Check!\n",
    "        #Could be commented out..\n",
    "        for i in range(len(self.audio)):\n",
    "            if(self.audio[i][:-4] != self.labels[i][0]):\n",
    "                print(self.audio[i])\n",
    "                print(self.labels[i][0])\n",
    "                break\n",
    "                \n",
    "        self.labels = [x[1] for x in self.labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        audio = self.audio[ind]\n",
    "        #Sanity Check\n",
    "        print(audio)\n",
    "        label = self.labels[ind]\n",
    "        #load audio when getting the item. If we do it in init, computer blue screens.\n",
    "        waveform, sample_rate = torchaudio.load(self.audio_dir + audio)\n",
    "        waveform = processor(waveform, sampling_rate = 16000,padding=True, device = device)\n",
    "        # waveform = waveform.to(device)\n",
    "        # label = label.to(device)\n",
    "        waveform['labels'] = label\n",
    "\n",
    "        return waveform\n",
    "        #return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_4001_6001_phnd_deb-1-0000000-0003226.wav\n",
      "{'input_values': [array([[ 1.5415961e-05,  1.5415961e-05,  1.5415961e-05, ...,\n",
      "         7.4268505e-03,  6.5004211e-03, -5.5431598e-03]], dtype=float32)], 'labels': 'ART MING GUAN'}\n",
      "app_4001_6001_phnd_deb-1-0010612-0028188.wav\n",
      "{'input_values': [array([[-0.00868657, -0.01794732, -0.03357484, ...,  0.28823635,\n",
      "         0.07350262, -0.15280704]], dtype=float32)], 'labels': 'OKAY SO UH I GUESS IT   S TIME FOR US TO DEBATE THE TOPIC UH TECHNOLOGY CREATES MORE PROBLEM THAN BENEFITS FOR THE SOCIETY SO PPB ERR ERR HAVE YOU UH DONE ANY UH RESEARCH ON THAT TOPIC'}\n",
      "app_4001_6001_phnd_deb-1-0028188-0031663.wav\n",
      "{'input_values': [array([[0.04162551, 0.03642268, 0.02750353, ..., 0.06540991, 0.11669502,\n",
      "        0.08027516]], dtype=float32)], 'labels': 'SO YA THE PROPONENT OPPOSITION'}\n",
      "app_4001_6001_phnd_deb-1-0034071-0048978.wav\n",
      "{'input_values': [array([[-0.01377804, -0.02656972, -0.01672996, ..., -0.02952164,\n",
      "        -0.03050562, -0.00098636]], dtype=float32)], 'labels': 'PPB OH YOU WANT TO YOU WANT SOME TIME TO DO THE DO THE THIS YOUR YOUR DO YOU DO YOU NEED SOMETIME TO DO THE PLANNING CAUSE I I OKAY I BASICALLY I JUST WROTE DOWN SOME SOME OF THE POINTS ON MY IN THE IF ME LAH STILL GOT SOME POINTS LAH BUT I THINK YOU SHOULD START FIRST SO YA I WAIT FOR YOU PPB'}\n",
      "app_4001_6001_phnd_deb-1-0053927-0055637.wav\n",
      "{'input_values': [array([[ 0.01575037, -0.02363743, -0.04530073, ...,  0.06498511,\n",
      "        -0.09650486, -0.11029059]], dtype=float32)], 'labels': 'OKAY SURE PPB'}\n"
     ]
    }
   ],
   "source": [
    "NSCSpeech = NSCSpeechDataset()\n",
    "for i in range(5):\n",
    "    print(NSCSpeech[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 24.9 MiB for an array with shape (1, 6516742) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m NSCSpeechDataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mfrom_list(NSCSpeech)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\miniconda3\\envs\\MML\\lib\\site-packages\\datasets\\arrow_dataset.py:901\u001b[0m, in \u001b[0;36mDataset.from_list\u001b[1;34m(cls, mapping, features, info, split)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[39mConvert a list of dicts to a :obj:`pyarrow.Table` to create a :class:`Dataset`.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[39m    :class:`Dataset`\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39m# for simplicity and consistency wrt OptimizedTypedSequence we do not use InMemoryTable.from_pylist here\u001b[39;00m\n\u001b[1;32m--> 901\u001b[0m mapping \u001b[39m=\u001b[39m {k: [r\u001b[39m.\u001b[39mget(k) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m mapping] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m mapping[\u001b[39m0\u001b[39m]} \u001b[39mif\u001b[39;00m mapping \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m    902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_dict(mapping, features, info, split)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\miniconda3\\envs\\MML\\lib\\site-packages\\datasets\\arrow_dataset.py:901\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[39mConvert a list of dicts to a :obj:`pyarrow.Table` to create a :class:`Dataset`.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[39m    :class:`Dataset`\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39m# for simplicity and consistency wrt OptimizedTypedSequence we do not use InMemoryTable.from_pylist here\u001b[39;00m\n\u001b[1;32m--> 901\u001b[0m mapping \u001b[39m=\u001b[39m {k: [r\u001b[39m.\u001b[39mget(k) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m mapping] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m mapping[\u001b[39m0\u001b[39m]} \u001b[39mif\u001b[39;00m mapping \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m    902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_dict(mapping, features, info, split)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\miniconda3\\envs\\MML\\lib\\site-packages\\datasets\\arrow_dataset.py:901\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    886\u001b[0m \u001b[39mConvert a list of dicts to a :obj:`pyarrow.Table` to create a :class:`Dataset`.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[39m    :class:`Dataset`\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39m# for simplicity and consistency wrt OptimizedTypedSequence we do not use InMemoryTable.from_pylist here\u001b[39;00m\n\u001b[1;32m--> 901\u001b[0m mapping \u001b[39m=\u001b[39m {k: [r\u001b[39m.\u001b[39mget(k) \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m mapping] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m mapping[\u001b[39m0\u001b[39m]} \u001b[39mif\u001b[39;00m mapping \u001b[39melse\u001b[39;00m {}\n\u001b[0;32m    902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mfrom_dict(mapping, features, info, split)\n",
      "Cell \u001b[1;32mIn [88], line 40\u001b[0m, in \u001b[0;36mNSCSpeechDataset.__getitem__\u001b[1;34m(self, ind)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39m#load audio when getting the item. If we do it in init, computer blue screens.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m waveform, sample_rate \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39mload(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_dir \u001b[39m+\u001b[39m audio)\n\u001b[1;32m---> 40\u001b[0m waveform \u001b[39m=\u001b[39m processor(waveform, sampling_rate \u001b[39m=\u001b[39;49m \u001b[39m16000\u001b[39;49m,padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, device \u001b[39m=\u001b[39;49m device)\n\u001b[0;32m     41\u001b[0m \u001b[39m# waveform = waveform.to(device)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m# label = label.to(device)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m waveform[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m label\n",
      "File \u001b[1;32mc:\\Users\\Justin\\miniconda3\\envs\\MML\\lib\\site-packages\\transformers\\models\\wav2vec2\\processing_wav2vec2.py:92\u001b[0m, in \u001b[0;36mWav2Vec2Processor.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou need to specify either an `audio` or `text` input to process.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[39mif\u001b[39;00m audio \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractor(audio, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[39mif\u001b[39;00m text \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(text, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\miniconda3\\envs\\MML\\lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:230\u001b[0m, in \u001b[0;36mWav2Vec2FeatureExtractor.__call__\u001b[1;34m(self, raw_speech, padding, max_length, truncation, pad_to_multiple_of, return_attention_mask, return_tensors, sampling_rate, **kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_normalize:\n\u001b[0;32m    225\u001b[0m     attention_mask \u001b[39m=\u001b[39m (\n\u001b[0;32m    226\u001b[0m         attention_mask\n\u001b[0;32m    227\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_strategies(padding, max_length\u001b[39m=\u001b[39mmax_length) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m PaddingStrategy\u001b[39m.\u001b[39mDO_NOT_PAD\n\u001b[0;32m    228\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     )\n\u001b[1;32m--> 230\u001b[0m     padded_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_values\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_mean_unit_var_norm(\n\u001b[0;32m    231\u001b[0m         padded_inputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_values\u001b[39;49m\u001b[39m\"\u001b[39;49m], attention_mask\u001b[39m=\u001b[39;49mattention_mask, padding_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_value\n\u001b[0;32m    232\u001b[0m     )\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     padded_inputs \u001b[39m=\u001b[39m padded_inputs\u001b[39m.\u001b[39mconvert_to_tensors(return_tensors)\n",
      "File \u001b[1;32mc:\\Users\\Justin\\miniconda3\\envs\\MML\\lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:98\u001b[0m, in \u001b[0;36mWav2Vec2FeatureExtractor.zero_mean_unit_var_norm\u001b[1;34m(input_values, attention_mask, padding_value)\u001b[0m\n\u001b[0;32m     96\u001b[0m         normed_input_values\u001b[39m.\u001b[39mappend(normed_slice)\n\u001b[0;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     normed_input_values \u001b[39m=\u001b[39m [(x \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mmean()) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(x\u001b[39m.\u001b[39mvar() \u001b[39m+\u001b[39m \u001b[39m1e-7\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m input_values]\n\u001b[0;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m normed_input_values\n",
      "File \u001b[1;32mc:\\Users\\Justin\\miniconda3\\envs\\MML\\lib\\site-packages\\transformers\\models\\wav2vec2\\feature_extraction_wav2vec2.py:98\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     96\u001b[0m         normed_input_values\u001b[39m.\u001b[39mappend(normed_slice)\n\u001b[0;32m     97\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m     normed_input_values \u001b[39m=\u001b[39m [(x \u001b[39m-\u001b[39m x\u001b[39m.\u001b[39mmean()) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msqrt(x\u001b[39m.\u001b[39;49mvar() \u001b[39m+\u001b[39m \u001b[39m1e-7\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m input_values]\n\u001b[0;32m    100\u001b[0m \u001b[39mreturn\u001b[39;00m normed_input_values\n",
      "File \u001b[1;32mc:\\Users\\Justin\\miniconda3\\envs\\MML\\lib\\site-packages\\numpy\\core\\_methods.py:233\u001b[0m, in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m    228\u001b[0m     arrmean \u001b[39m=\u001b[39m arrmean \u001b[39m/\u001b[39m rcount\n\u001b[0;32m    230\u001b[0m \u001b[39m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[39m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[39m# not a scalar.\u001b[39;00m\n\u001b[1;32m--> 233\u001b[0m x \u001b[39m=\u001b[39m asanyarray(arr \u001b[39m-\u001b[39;49m arrmean)\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(arr\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, (nt\u001b[39m.\u001b[39mfloating, nt\u001b[39m.\u001b[39minteger)):\n\u001b[0;32m    236\u001b[0m     x \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mmultiply(x, x, out\u001b[39m=\u001b[39mx)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 24.9 MiB for an array with shape (1, 6516742) and data type float32"
     ]
    }
   ],
   "source": [
    "NSCSpeechDataset = Dataset.from_list(NSCSpeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UR4qfYrVoO4v",
    "gg3-yJ8tok34",
    "R9v5ewZDMpYA",
    "Ly4mjUUUuJhy",
    "HLad4pChcuvX",
    "tUThsowyQdN7",
    "IBwunYpyugFg",
    "kH0RAbCaMl9a",
    "qpYExu4vT4_g",
    "MY69hgxUXhTI",
    "M2H4EEj-sD32"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "50a168d2e944e50d64023c982abdec6febc400ad89db390dba97250ae813c14d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
