{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QlY7CYLmIGGM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-wkEEfw8KkG",
    "outputId": "0f7af032-8e1b-4ea0-cc2d-6c432e2ad19d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rvitiell/miniconda3/envs/dl_proj/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "#Dataset Imports\n",
    "import csv\n",
    "from IPython.display import Audio, display\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m9I2WS788g7R"
   },
   "outputs": [],
   "source": [
    "#Set directory to the Audios folder in MSP dataset\n",
    "AUDIO_ROOT = '/data/NO-BACKUP/rvitiell-data/MSP/Audios/'\n",
    "#Set path to labels_consensus in MSP dataset\n",
    "LABELS_DIR = '/data/NO-BACKUP/rvitiell-data/MSP/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvDQ0sB79JCL",
    "outputId": "de1be311-32c5-4a1b-8b8d-89593dbfc618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 167814])\n",
      "16000\n"
     ]
    }
   ],
   "source": [
    "#Load the directory\n",
    "names = sorted(os.listdir(AUDIO_ROOT))\n",
    "data1 = AUDIO_ROOT + names[0]\n",
    "#torchaudio.load requires you to install some programs if you get 'No audio I/O backend is available' error\n",
    "#https://stackoverflow.com/questions/62543843/cannot-import-torch-audio-no-audio-backend-is-available\n",
    "waveform, sample_rate = torchaudio.load(data1)\n",
    "print(waveform.shape)\n",
    "print(sample_rate)\n",
    "\n",
    "#Load label csv file\n",
    "with open(LABELS_DIR + 'labels_concensus.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    labels = sorted(list(reader)[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "p8hY8C6A9L9J"
   },
   "outputs": [],
   "source": [
    "model_name_or_path = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "pooling_mode = \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8NjeePa9L_u",
    "outputId": "105ad78c-ba0e-43f3-a0d0-5d853f697e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target sampling rate: 16000\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(f\"The target sampling rate: {target_sampling_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gjn_AFN-9XJk"
   },
   "outputs": [],
   "source": [
    "# {'A':1, 'S':2, 'H':3, 'U':4, 'F':5, 'D':6, 'C':7, 'N':8, 'O':9}\n",
    "EMOMAP = {'A':0, 'S':1, 'H':2,'D':3,'N':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Tmy5TW929MCK"
   },
   "outputs": [],
   "source": [
    "class MSPDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    #Initialize the dataset based on the recommended split in MSP dataset.\n",
    "    def __init__(self, train = False, valid = False, test1 = False, test2 = False): \n",
    "        \n",
    "        self.audio_dir = AUDIO_ROOT\n",
    "        self.labels_dir = LABELS_DIR\n",
    "        self.audio_names = sorted(names)\n",
    "        self.labels_list = labels\n",
    "        self.EMOMAP = EMOMAP\n",
    "\n",
    "        self.audio = []\n",
    "        self.labels = []\n",
    "        \n",
    "        #What type of dataset are we making\n",
    "        setType = 'Train'\n",
    "        if valid:\n",
    "            setType = 'Validation'\n",
    "        elif test1:\n",
    "            setType = 'Test1'\n",
    "        elif test2:\n",
    "            setType = 'Test2'\n",
    "        print(setType)       \n",
    "        \n",
    "        #Sanitycheck1\n",
    "        assert(len(self.audio_names) == len(self.labels_list))\n",
    "        \n",
    "        for i in tqdm(range(0, len(self.audio_names))):\n",
    "            assert(self.audio_names[i] == self.labels_list[i][0])\n",
    "            if self.labels_list[i][7] != setType or self.labels_list[i][1] == 'X' or self.labels_list[i][1] not in self.EMOMAP.keys():\n",
    "                continue\n",
    "            #43 Audio files from 1904 podcast seems to be broken. Torchaudio load returns 'no data chunk'\n",
    "            if self.labels_list[i][0].startswith('MSP-PODCAST_1904'):\n",
    "                continue\n",
    "            self.audio.append(self.audio_dir + self.audio_names[i])\n",
    "            self.labels.append(self.EMOMAP[self.labels_list[i][1]])         \n",
    "        \n",
    "        self.length = len(self.audio)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        audio = self.audio[ind]\n",
    "        label = self.labels[ind]\n",
    "        #load audio when getting the item. If we do it in init, computer blue screens.\n",
    "        waveform, sample_rate = torchaudio.load(audio)\n",
    "\n",
    "        waveform = processor(waveform, sampling_rate = target_sampling_rate,padding=True, device = device)\n",
    "\n",
    "        # waveform = waveform.to(device)\n",
    "        # label = label.to(device)\n",
    "        waveform['labels'] = label\n",
    "\n",
    "        return waveform\n",
    "        #return waveform, label\n",
    "    \n",
    "    # def collate_fn(self, batch):\n",
    "    #     batch_audio = [x[0].reshape(-1) for x in batch]\n",
    "\n",
    "    #     audio_lengths = torch.LongTensor([len(x) for x in batch_audio])\n",
    "    #     batch_audio = pad_sequence(batch_audio, padding_value=0.0, batch_first = True)\n",
    "    #     batch_label = [x[1] for x in batch]\n",
    "        \n",
    "    #     return batch_audio, audio_lengths, torch.tensor(batch_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v7Habdl69MEJ",
    "outputId": "5bee56f2-f518-48a9-aabe-427682294340"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nH25clrO9-Z9",
    "outputId": "b189a617-1e6d-4af6-cad0-9ea75150c5a8"
   },
   "outputs": [],
   "source": [
    "# # Create Dataset objects.\n",
    "# train_data = MSPDataset(train = True) \n",
    "# val_data = MSPDataset(valid = True) \n",
    "# test_data = MSPDataset(test1 = True)\n",
    "\n",
    "# print(train_data.__len__())\n",
    "# print(val_data.__len__())\n",
    "# print(test_data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "6D5_H3tr-ztj"
   },
   "outputs": [],
   "source": [
    "# !huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "f5530f0932c24dd7b54f981772a9f3f9",
      "29a7cb8c6da34e788d1172517c9d8222",
      "024082b6ed9741408ccce999bf25fbfa",
      "f7f9135e0cc84b31934c60404a52d2ee",
      "3f3f300e2b554ca98631efc305d6f27f",
      "12f1c2e949ce4697a1739bba1a8368d8",
      "7e54ae5239cb46a48f9f34a669c49e6e",
      "fbf583ff90d24bc1bea06dd3f24d0579",
      "2ce546a57627491bb43e15a6e4cf22e1",
      "8da6347979dc41018d604c40d15bda67",
      "482d1cebc1394d74b2f2fffac8a05d5f"
     ]
    },
    "id": "M3Ztd3X59-d1",
    "outputId": "2679351b-82c2-4fdb-a7d1-7fb3ff94151d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dlproject--msp_train_xlsr-1a4e4daddf3eb50e\n",
      "Found cached dataset parquet (/home/rvitiell/.cache/huggingface/datasets/dlproject___parquet/dlproject--msp_train_xlsr-1a4e4daddf3eb50e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 12.45it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "# train_dataset = Dataset.from_list(train_data)\n",
    "# val_dataset = Dataset.from_list(val_data)\n",
    "train_dataset = load_dataset(\"dlproject/msp_train_xlsr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4Q2ckHiVtCDo"
   },
   "outputs": [],
   "source": [
    "# train_dataset.push_to_hub(\"dlproject/msp_train_xlsr\")\n",
    "# val_dataset.push_to_hub(\"dlproject/msp_val_xlsr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BrwMLrWoSPBt"
   },
   "outputs": [],
   "source": [
    "#val_dataset = Dataset.from_list(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "c32f90b1fd8444369105ff2e7632d58e",
      "50dcb0f13ef24aa5986d747efe05bcab",
      "323c38f8546648dd95362a588ba2cf13",
      "1217bd7add68406f987a865f5b682d02",
      "2c40140478c24ed68c9233d9d3218648",
      "803764ec96494fc9ac8e07bddd80c2e2",
      "8b905ad0075c4047a0770459ac87acc7",
      "7d47d61d1d9843c0bccfa581df66266e",
      "32f994f9a93346c7813944fa515e1bd3",
      "5c3e551475c543eeb8d1093bb270d225",
      "0cdaa54e14b24b3d9033d20ad401a374"
     ]
    },
    "id": "VkCN7YNq9-f_",
    "outputId": "fad38754-5731-4219-8e1a-9e138048ed57"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dlproject--msp_val_xlsr-ab4d76d87cbcc388\n",
      "Found cached dataset parquet (/home/rvitiell/.cache/huggingface/datasets/dlproject___parquet/dlproject--msp_val_xlsr-ab4d76d87cbcc388/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 1/1 [00:00<00:00, 66.18it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = load_dataset(\"dlproject/msp_val_xlsr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "16_tn0tYIiLI"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset['train']\n",
    "val_dataset = val_dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qtdHquSH_5Yx"
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # all_features = [torch.tensor(feature['input_values']) for feature in features]\n",
    "        #print(typeall_features)\n",
    "        #padded_inputs = pad_sequence(all_features)\n",
    "        #input_features = [{\"input_values\": torch.tensor(feature[\"input_values\"][0][0]), \"attention_mask\":torch.tensor(feature[\"attention_mask\"][0])} for feature in features]\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"][0][0]} for feature in features]\n",
    "\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        #input_lengths = [len(x['input_values']) for x in input_features]\n",
    "        # print(input_lengths)\n",
    "\n",
    "        #print(input_features)\n",
    "\n",
    "        #input_features = input_features.to(device)\n",
    "        #label_features = label_features.to(device)        \n",
    "        #print(input_features[0]['input_values'].device)\n",
    "    \n",
    "        #d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # batch = {}\n",
    "        # batch['input_values'] = torch.stack([features['input_values'] for features in input_features])\n",
    "        # batch['attention_mask'] = torch.stack([features['attention_mask'] for features in input_features])\n",
    "        \n",
    "        batch['input_values'] = batch['input_values'].to(device)\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features,device=device)\n",
    "\n",
    "        # print(batch['labels'])\n",
    "        # print(batch['input_values'].device)\n",
    "        # print(batch['labels'].device)\n",
    "\n",
    "        assert len(batch['input_values'])==len(batch[\"labels\"])\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Kt94mQnn_7q2"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "osaqu2N1_7t3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    f1 = F1Score(num_classes=5,average='weighted')\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item(), \"f1\": f1(torch.tensor(preds), torch.tensor(p.label_ids))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lSI3gl3S_7xa"
   },
   "outputs": [],
   "source": [
    "# config\n",
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(EMOMAP),\n",
    "    label2id=EMOMAP,\n",
    "    id2label={i: label for label,i in EMOMAP.items()},\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ubsh_tElHnmA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from transformers.file_utils import ModelOutput\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xxcwc0ou_7zb",
    "outputId": "f2622171-5d50-4754-dcd3-3eac7b454e6f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english were not used when initializing Wav2Vec2ForSpeechClassification: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSpeechClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "    \"jonatasgrosman/wav2vec2-large-xlsr-53-english\",\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "IPBZhdBq_71E"
   },
   "outputs": [],
   "source": [
    "model.freeze_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8K-e1GKBAFFJ"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch\",\n",
    "    per_device_train_batch_size=3,\n",
    "    per_device_eval_batch_size=3,\n",
    "    gradient_accumulation_steps=128,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=10.0,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    save_total_limit=2,\n",
    "    dataloader_pin_memory=False,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gh5S8h6sAFHG"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    is_apex_available,\n",
    ")\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        #model = model.to(device)\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        if self.use_cuda_amp:\n",
    "            with autocast():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "        else:\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.use_cuda_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "hM9C-VqAdpJX",
    "outputId": "20f21157-2d76-43ef-8930-cf2884fbf50b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "HefPxe6Hdiic"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGnGtYQ_lZxu",
    "outputId": "b99908a6-c860-44e9-b8bd-aa1d93854d2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iLWlKYd_AFJg",
    "outputId": "0767e053-d4de-4b4f-ee51-b09cd28817c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AYyEgeShx96A",
    "outputId": "0f56ac9b-0c06-4dc6-d1db-fd54861a7454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 14 00:38:33 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.86       Driver Version: 470.86       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:88:00.0 Off |                  Off |\n",
      "| 30%   33C    P2    76W / 300W |   1997MiB / 48685MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A     37404      C   ...3/envs/dl_proj/bin/python     1995MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "bzmoqyMri34K"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zCYQOw7Oi_G2",
    "outputId": "dd043493-b4eb-4c34-8a71-dd6f8db32bce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 678
    },
    "id": "JzTYOeNTAFLf",
    "outputId": "a51f3f75-19ae-46a0-d6bb-84113c5631ec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 29939\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 384\n",
      "  Gradient Accumulation steps = 128\n",
      "  Total optimization steps = 770\n",
      "  Number of trainable parameters = 312283269\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrosavitiello\u001b[0m (\u001b[33mvarf\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rvitiell/classes/dl/proj/11-785-Fall2022-Project/wandb/run-20221214_003835-1fjuhrn8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/varf/huggingface/runs/1fjuhrn8\" target=\"_blank\">/data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch</a></strong> to <a href=\"https://wandb.ai/varf/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='730' max='770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [730/770 12:00:52 < 39:36, 0.02 it/s, Epoch 9.46/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.111049</td>\n",
       "      <td>0.587186</td>\n",
       "      <td>0.541116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.046100</td>\n",
       "      <td>1.046569</td>\n",
       "      <td>0.608479</td>\n",
       "      <td>0.560838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.868900</td>\n",
       "      <td>1.060587</td>\n",
       "      <td>0.617878</td>\n",
       "      <td>0.572201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>1.143382</td>\n",
       "      <td>0.608479</td>\n",
       "      <td>0.571362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>1.222103</td>\n",
       "      <td>0.587186</td>\n",
       "      <td>0.559739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.658900</td>\n",
       "      <td>1.334653</td>\n",
       "      <td>0.577019</td>\n",
       "      <td>0.561430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.538900</td>\n",
       "      <td>1.481356</td>\n",
       "      <td>0.561289</td>\n",
       "      <td>0.543673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.457100</td>\n",
       "      <td>1.569521</td>\n",
       "      <td>0.575676</td>\n",
       "      <td>0.558558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.457100</td>\n",
       "      <td>1.664435</td>\n",
       "      <td>0.571264</td>\n",
       "      <td>0.554753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5213\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-77\n",
      "Configuration saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-77/config.json\n",
      "Model weights saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-77/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5213\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-154\n",
      "Configuration saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-154/config.json\n",
      "Model weights saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-154/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5213\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-231\n",
      "Configuration saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-231/config.json\n",
      "Model weights saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-231/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-77] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5213\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-308\n",
      "Configuration saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-308/config.json\n",
      "Model weights saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-308/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-231] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5213\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-385\n",
      "Configuration saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-385/config.json\n",
      "Model weights saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-385/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-308] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5213\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-462\n",
      "Configuration saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-462/config.json\n",
      "Model weights saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-462/pytorch_model.bin\n",
      "Saving model checkpoint to /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-539\n",
      "Configuration saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-539/config.json\n",
      "Model weights saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-539/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-462] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5213\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-616\n",
      "Configuration saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-616/config.json\n",
      "Model weights saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-616/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-539] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5213\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-693\n",
      "Configuration saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-693/config.json\n",
      "Model weights saved in /data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-693/pytorch_model.bin\n",
      "Deleting older checkpoint [/data/NO-BACKUP/rvitiell-data/MSP/models/xlsr-fixed-10epoch/checkpoint-616] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BqbWpytON_P3"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "dl_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "024082b6ed9741408ccce999bf25fbfa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e54ae5239cb46a48f9f34a669c49e6e",
      "placeholder": "​",
      "style": "IPY_MODEL_12f1c2e949ce4697a1739bba1a8368d8",
      "value": "100%"
     }
    },
    "0cdaa54e14b24b3d9033d20ad401a374": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1217bd7add68406f987a865f5b682d02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32f994f9a93346c7813944fa515e1bd3",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7d47d61d1d9843c0bccfa581df66266e",
      "value": 1
     }
    },
    "12f1c2e949ce4697a1739bba1a8368d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "29a7cb8c6da34e788d1172517c9d8222": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c40140478c24ed68c9233d9d3218648": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0cdaa54e14b24b3d9033d20ad401a374",
      "placeholder": "​",
      "style": "IPY_MODEL_5c3e551475c543eeb8d1093bb270d225",
      "value": " 1/1 [00:00&lt;00:00, 38.64it/s]"
     }
    },
    "2ce546a57627491bb43e15a6e4cf22e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "323c38f8546648dd95362a588ba2cf13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b905ad0075c4047a0770459ac87acc7",
      "placeholder": "​",
      "style": "IPY_MODEL_803764ec96494fc9ac8e07bddd80c2e2",
      "value": "100%"
     }
    },
    "32f994f9a93346c7813944fa515e1bd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f3f300e2b554ca98631efc305d6f27f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_482d1cebc1394d74b2f2fffac8a05d5f",
      "placeholder": "​",
      "style": "IPY_MODEL_8da6347979dc41018d604c40d15bda67",
      "value": " 1/1 [00:00&lt;00:00, 39.53it/s]"
     }
    },
    "482d1cebc1394d74b2f2fffac8a05d5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50dcb0f13ef24aa5986d747efe05bcab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5c3e551475c543eeb8d1093bb270d225": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d47d61d1d9843c0bccfa581df66266e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7e54ae5239cb46a48f9f34a669c49e6e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "803764ec96494fc9ac8e07bddd80c2e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b905ad0075c4047a0770459ac87acc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8da6347979dc41018d604c40d15bda67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c32f90b1fd8444369105ff2e7632d58e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_323c38f8546648dd95362a588ba2cf13",
       "IPY_MODEL_1217bd7add68406f987a865f5b682d02",
       "IPY_MODEL_2c40140478c24ed68c9233d9d3218648"
      ],
      "layout": "IPY_MODEL_50dcb0f13ef24aa5986d747efe05bcab"
     }
    },
    "f5530f0932c24dd7b54f981772a9f3f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_024082b6ed9741408ccce999bf25fbfa",
       "IPY_MODEL_f7f9135e0cc84b31934c60404a52d2ee",
       "IPY_MODEL_3f3f300e2b554ca98631efc305d6f27f"
      ],
      "layout": "IPY_MODEL_29a7cb8c6da34e788d1172517c9d8222"
     }
    },
    "f7f9135e0cc84b31934c60404a52d2ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ce546a57627491bb43e15a6e4cf22e1",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fbf583ff90d24bc1bea06dd3f24d0579",
      "value": 1
     }
    },
    "fbf583ff90d24bc1bea06dd3f24d0579": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
