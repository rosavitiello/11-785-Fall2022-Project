{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ede9fd-1356-4506-935a-cfedbd2df445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "948498bf-3379-4655-837b-3308187a5ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: transformers==4.24.0 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (4.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from transformers==4.24.0) (2022.10.31)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from transformers==4.24.0) (3.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from transformers==4.24.0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from transformers==4.24.0) (1.23.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from transformers==4.24.0) (0.11.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from transformers==4.24.0) (4.64.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from transformers==4.24.0) (0.12.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from transformers==4.24.0) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from transformers==4.24.0) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.24.0) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from requests->transformers==4.24.0) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from requests->transformers==4.24.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from requests->transformers==4.24.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from requests->transformers==4.24.0) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c051c0b7-e477-481c-95ec-0b8766f75da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: scikit-learn in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (1.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from scikit-learn) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages (from scikit-learn) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaf08208-cb4d-4a41-8043-2bd3bc356e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/emotion/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "#Dataset Imports\n",
    "import csv\n",
    "from IPython.display import Audio, display\n",
    "import torchaudio\n",
    "from transformers import Wav2Vec2FeatureExtractor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b20a923-ca54-4dfa-a6e3-58537d802caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n"
     ]
    }
   ],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e598f2b6-4607-4776-b822-70902382684c",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226bbd0d-e991-43e4-93e7-a77daea93184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'A':1, 'S':2, 'H':3, 'U':4, 'F':5, 'D':6, 'C':7, 'N':8, 'O':9}\n",
    "EMOMAP = {'A':0, 'S':1, 'H':2,'D':3,'N':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d06929eb-2b28-467d-9db7-d4551c410b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NSC_ASR_Root = '/home/ubuntu/data/NSC/'\n",
    "NSC_test_dataset = '/home/ubuntu/data/NSC/NSC_part5_labelled_emotion/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57240d9-0b83-47b3-8dd5-e68b4acd4a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "model_name_or_path = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name_or_path,)\n",
    "target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "print(target_sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "072ca283-93c7-4b1a-b105-6dfb37b43ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSCSpeechDatasetASR(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self): \n",
    "        \n",
    "        self.audio_dir = NSC_ASR_Root + 'top20k/'\n",
    "        self.labels_dir = NSC_ASR_Root + 'text'\n",
    "        self.audio = []\n",
    "        self.labels = []\n",
    "        self.audio = sorted(os.listdir(self.audio_dir)[0:100])\n",
    "\n",
    "        #Get the correct labels for the 20000 that we have.\n",
    "        with open(self.labels_dir) as f:\n",
    "            lines = f.readlines()\n",
    "            start = False\n",
    "            for l in lines:\n",
    "                idx = int(l[4:8])\n",
    "                #Start at APP_4001 and take 20000 from there\n",
    "                if idx >= 4001 and len(self.labels) != len(self.audio):\n",
    "                    #Remove new line and extract transcript\n",
    "                    self.labels.append(l[:-1].split(\" \", 1))\n",
    "        print(len(self.audio))\n",
    "        print(len(self.labels))\n",
    "        assert(len(self.audio) == len(self.labels))\n",
    "        self.length = len(self.audio)\n",
    "        #Sanity Check!\n",
    "        #Could be commented out..\n",
    "        # for i in range(len(self.audio)):\n",
    "        #     if(self.audio[i][:-4] != self.labels[i][0]):\n",
    "        #         print(self.audio[i])\n",
    "        #         print(self.labels[i][0])\n",
    "        #         break\n",
    "                \n",
    "        self.labels = [x[1] for x in self.labels][0:100]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        audio = self.audio[ind]\n",
    "        #Sanity Check\n",
    "        print(audio)\n",
    "        label = self.labels[ind]\n",
    "        #load audio when getting the item. If we do it in init, computer blue screens.\n",
    "        waveform, sample_rate = torchaudio.load(self.audio_dir + audio)\n",
    "        waveform = processor(waveform, sampling_rate = 16000,padding=True, return_tensors=\"pt\", device = device)\n",
    "        # waveform = waveform.to(device)\n",
    "        # label = label.to(device)\n",
    "        waveform['labels'] = label\n",
    "\n",
    "        return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab487e40-96e6-43db-9312-aaefd612a8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSCDatasetEmotion(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self): \n",
    "        \n",
    "        audio_dir = NSC_test_dataset\n",
    "        #quick way of looping subdirectories. Dataset only has 4 categories. \n",
    "        subdirectory = [('Anger/', 0), ('Sad/', 1), ('Happy/', 2), ('Neutral/', 4)]\n",
    "        self.audio = []\n",
    "        self.labels = []\n",
    "        for sub, label in subdirectory:\n",
    "            print(audio_dir + sub)\n",
    "            NSCaudios = os.listdir(audio_dir + sub)\n",
    "            self.audio += [audio_dir + sub + x for x in NSCaudios[0:20]]\n",
    "            self.labels += [label]*len(NSCaudios[0:20]) \n",
    "        #Sanitycheck1\n",
    "        assert(len(self.audio) == len(self.labels))\n",
    "        self.length = len(self.audio)\n",
    "        \n",
    "        #self.audio = \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        audio = self.audio[ind]\n",
    "        label = self.labels[ind]\n",
    "        #load audio when getting the item. If we do it in init, computer blue screens.\n",
    "        waveform, sample_rate = torchaudio.load(audio)\n",
    "        waveform = processor(waveform, sampling_rate = 16000,padding=True, return_tensors=\"pt\", device = device)\n",
    "        # waveform = waveform.to(device)\n",
    "        # label = label.to(device)\n",
    "        waveform['labels'] = label\n",
    "\n",
    "        return waveform\n",
    "        #return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b758678b-febb-434a-b3f5-d2178154c524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/NSC/NSC_part5_labelled_emotion/Anger/\n",
      "/home/ubuntu/data/NSC/NSC_part5_labelled_emotion/Sad/\n",
      "/home/ubuntu/data/NSC/NSC_part5_labelled_emotion/Happy/\n",
      "/home/ubuntu/data/NSC/NSC_part5_labelled_emotion/Neutral/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_values': tensor([[[0.2524, 0.3185, 0.3613,  ..., 0.2399, 0.2616, 0.2622]]]), 'labels': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NSCTest = NSCDatasetEmotion()\n",
    "NSCTest[0]\n",
    "# NSCtest_dataset = Dataset.from_list(NSCTest)\n",
    "\n",
    "# #stratified sort to train/test splits. Requires encoding the columns to classes first.\n",
    "# NSCtest_dataset2 = NSCtest_dataset.class_encode_column('labels')\n",
    "# #NSCtest_dataset2.train_test_split(test_size = 0.1, stratify_by_column = 'labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36252570-b7a7-41d8-9ba7-de2d8040bce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "{'input_values': tensor([[[0.2524, 0.3185, 0.3613,  ..., 0.2399, 0.2616, 0.2622]]]), 'labels': 0}\n",
      "{'input_values': tensor([[[-0.0588, -0.0411, -0.0239,  ...,  0.0412,  0.0393,  0.0015]]]), 'labels': 0}\n",
      "{'input_values': tensor([[[-0.1709, -0.1947, -0.1973,  ..., -0.0908,  0.0167,  0.0577]]]), 'labels': 0}\n",
      "{'input_values': tensor([[[-0.0713, -0.3233, -0.4515,  ...,  1.1354,  1.3906,  1.7724]]]), 'labels': 0}\n",
      "{'input_values': tensor([[[ 0.0322,  0.0168, -0.0074,  ...,  1.2563,  1.2658,  1.2795]]]), 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "NSCSpeech = NSCSpeechDatasetASR()\n",
    "for i in range(5):\n",
    "    print(NSCTest[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98fde313-e30a-4d54-9a18-19bd3b0ce3de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dlproject--msp_train_hubert-d9cb93d944f5240e\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/dlproject___parquet/dlproject--msp_train_hubert-d9cb93d944f5240e/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  5.85it/s]\n",
      "Using custom data configuration dlproject--msp_val_hubert-89eed54aad19cdf2\n",
      "Found cached dataset parquet (/home/ubuntu/.cache/huggingface/datasets/dlproject___parquet/dlproject--msp_val_hubert-89eed54aad19cdf2/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 38.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "MSP_podcast_train = load_dataset(\"dlproject/msp_train_hubert\")\n",
    "MSP_podcast_val = load_dataset(\"dlproject/msp_val_hubert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e9cbb05-0e41-463f-a455-d84dfd23315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NSCSpeech.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7dbc400b-8b51-45c9-ad20-8d4fbdc056fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class CombinedDataForDomainAdaptation(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self): \n",
    "        self.msp_data = MSP_podcast_train\n",
    "        self.nsc_data = NSCSpeech\n",
    "        self.length = MSP_podcast_train['train'].num_rows + NSCSpeech.length\n",
    "        \n",
    "        all_audio_records = []\n",
    "        for audio_record in self.msp_data['train']:\n",
    "            audio_record['src'] = \"eng\"\n",
    "            all_audio_records.append(audio_record)\n",
    "        \n",
    "        for audio_record in self.nsc_data:\n",
    "            audio_record['src'] = \"sing_eng\"\n",
    "            all_audio_records.append(audio_record)\n",
    "        \n",
    "        self.audio = all_audio_records\n",
    "            \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        \n",
    "        audio = self.audio[ind]['input_values']\n",
    "            \n",
    "        label = self.audio[ind]['labels']\n",
    "        return audio, label\n",
    "        #return waveform, label\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        for record in batch:\n",
    "            if record['src'] == 'eng':\n",
    "        batch_audio = [x[0].reshape(-1) for x in batch]\n",
    "'''\n",
    "\n",
    "def create_combined_batches(batch_size):\n",
    "    #num_batches = (MSP_podcast_train['train'].num_rows + NSCSpeech.length)//batch_size\n",
    "    num_batches = (100 + NSCSpeech.length)//batch_size\n",
    "    combined_batches = []\n",
    "    for i in range(num_batches):\n",
    "        batch = []\n",
    "        #batch.append(MSP_podcast_train['train'][0:batch_size//2])\n",
    "        batch_input_values = []\n",
    "        batch_labels = []\n",
    "        for record in range(batch_size//2):\n",
    "            batch_input_values.append(torch.tensor(MSP_podcast_train['train'][record]['input_values']))\n",
    "            batch_labels.append(MSP_podcast_train['train']['labels'])\n",
    "            batch.append({'input_values':batch_input_values, 'labels':batch_labels})\n",
    "        for record in range(batch_size//2):\n",
    "            batch_input_values.append(NSCSpeech[record]['input_values'])\n",
    "            batch_labels.append(NSCSpeech[record]['labels'])\n",
    "            batch.append({'input_values':batch_input_values, 'labels':batch_labels})\n",
    "        combined_batches.append(batch)\n",
    "    return combined_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e024121-b82a-4b54-bf65-1246ff1eb44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4001_6001_phnd_deb-1-0783461-0786319.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6007_phnd_deb-1-0900535-0906250.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-0061113-0066908.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n",
      "app_4004_6008_phnd_deb-2-1156980-1166245.wav\n"
     ]
    }
   ],
   "source": [
    "combined_batches = create_combined_batches(8)\n",
    "#train_dataset = load_dataset(\"dlproject/msp_train_hubert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cccbf5d0-04b0-421f-8cf5-66ab50d0dbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for proccessing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # all_features = [torch.tensor(feature['input_values']) for feature in features]\n",
    "        #print(typeall_features)\n",
    "        #padded_inputs = pad_sequence(all_features)\n",
    "        #input_features = [{\"input_values\": torch.tensor(feature[\"input_values\"][0][0]), \"attention_mask\":torch.tensor(feature[\"attention_mask\"][0])} for feature in features]\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"][0][0]} for feature in features]\n",
    "\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        #input_lengths = [len(x['input_values']) for x in input_features]\n",
    "        # print(input_lengths)\n",
    "\n",
    "        #print(input_features)\n",
    "\n",
    "        #input_features = input_features.to(device)\n",
    "        #label_features = label_features.to(device)        \n",
    "        #print(input_features[0]['input_values'].device)\n",
    "    \n",
    "        #d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # batch = {}\n",
    "        # batch['input_values'] = torch.stack([features['input_values'] for features in input_features])\n",
    "        # batch['attention_mask'] = torch.stack([features['attention_mask'] for features in input_features])\n",
    "        \n",
    "        batch['input_values'] = batch['input_values'].to(device)\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features,device=device)\n",
    "\n",
    "        # print(batch['labels'])\n",
    "        # print(batch['input_values'].device)\n",
    "        # print(batch['labels'].device)\n",
    "\n",
    "        assert len(batch['input_values'])==len(batch[\"labels\"])\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b689923-5984-4e6e-8027-9517ef29bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "752fdd60-c8e1-40f7-937d-677589c410f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import EvalPrediction\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    f1 = F1Score(num_classes=5,average='weighted')\n",
    "    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item(), \"f1\": f1(torch.tensor(preds), torch.tensor(p.label_ids))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f554f6c-ee65-480b-b696-6cd1f96e21ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "pooling_mode = \"mean\"\n",
    "model_name_or_path = \"facebook/hubert-base-ls960\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(EMOMAP),\n",
    "    label2id=EMOMAP,\n",
    "    id2label={i: label for label,i in EMOMAP.items()},\n",
    "    finetuning_task=\"wav2vec2_clf\",\n",
    ")\n",
    "setattr(config, 'pooling_mode', pooling_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e69304f5-dcd7-417e-a1c9-bfb9b7ac99a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['projector.bias', 'projector.weight', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of HubertForCTC were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['lm_head.bias', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import HubertModel, HubertForCTC,HubertForSequenceClassification\n",
    "feature_model = HubertModel.from_pretrained(\n",
    "    \"facebook/hubert-base-ls960\",\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "emotion_classifier_model = HubertForSequenceClassification.from_pretrained(\n",
    "    \"facebook/hubert-base-ls960\",\n",
    "    config=config,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "emotion_classifier_model.freeze_feature_extractor()\n",
    "\n",
    "ctc_model_for_asr = HubertForCTC.from_pretrained(\n",
    "    \"facebook/hubert-base-ls960\",\n",
    "     config=config,\n",
    "     ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63691188-cf88-4d84-8028-29fd20992777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Union\n",
    "\n",
    "import torch\n",
    "from packaging import version\n",
    "from torch import nn\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    is_apex_available,\n",
    ")\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "class CTCTrainer(Trainer):\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        #model = model.to(device)\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        if self.use_cuda_amp:\n",
    "            with autocast():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "        else:\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.use_cuda_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.detach()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3052488-061b-4bf0-b4f2-6a923d9cc52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "203d31ec-a981-4642-91ba-31195e944f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# from functions import ReverseLayerF\n",
    "\n",
    "\n",
    "class DomainAdaptationModelForAccentedSpeech(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DomainAdaptationModelForAccentedSpeech, self).__init__()\n",
    "        \n",
    "        # Feature Extractor Model\n",
    "        self.feature = feature_model\n",
    "        \n",
    "        # Emotion Classification Model\n",
    "        self.class_classifier = emotion_classifier_model\n",
    "        \n",
    "        # ASR Model\n",
    "        self.asr_model = ctc_model_for_asr\n",
    "\n",
    "        self.domain_classifier = nn.Sequential()\n",
    "        self.domain_classifier.add_module('d_fc1', nn.Linear(50 * 4 * 4, 100))\n",
    "        self.domain_classifier.add_module('d_bn1', nn.BatchNorm1d(100))\n",
    "        self.domain_classifier.add_module('d_relu1', nn.ReLU(True))\n",
    "        self.domain_classifier.add_module('d_fc2', nn.Linear(100, 2))\n",
    "        self.domain_classifier.add_module('d_softmax', nn.LogSoftmax(dim=1))\n",
    "\n",
    "    def forward(self, input_data, alpha):\n",
    "        #input_data = input_data.expand(input_data.data.shape[0], 3, 28, 28)\n",
    "        feature = self.feature(input_data)\n",
    "        #feature = feature.view(-1, 50 * 4 * 4)\n",
    "        reverse_feature = ReverseLayerF.apply(feature, alpha)\n",
    "        class_output = self.class_classifier(feature)\n",
    "        domain_output = self.domain_classifier(reverse_feature)\n",
    "\n",
    "        return class_output, domain_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2d75c250-523c-4ddb-af35-a063f7c10e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "DANN = DomainAdaptationModelForAccentedSpeech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c677c-3b7c-4f58-875c-930204da6ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def get_batch_iterator():\n",
    "    source_loader = MSP_podcast_train\n",
    "\n",
    "    source_loader = DataLoader(source_loader['train'], batch_size=1)\n",
    "    # print(\"SOURCE:\")\n",
    "    # for i,d in enumerate(source_loader):\n",
    "    #     print(d)\n",
    "    #     if i>2:\n",
    "    #         break\n",
    "            \n",
    "            \n",
    "    #print(source_loader)\n",
    "    target_loader = NSCSpeech\n",
    "    \n",
    "    # print(\"TARGET:\")\n",
    "    # for i,d in enumerate(target_loader):\n",
    "    #     print(d)\n",
    "    #     if i>2:\n",
    "    #         break\n",
    "   # print(target_loader)\n",
    "  \n",
    "    batches = zip(source_loader, target_loader)\n",
    "    n_batches = min(len(source_loader), len(target_loader))\n",
    "\n",
    "    print(list(batches))\n",
    "    return batches, n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b51b07-e43b-46ec-ab63-38745513ffe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batches, n_batches, model, discriminator, optimizer, criterion, device):\n",
    "    model.train()\n",
    "\n",
    "    total_domain_loss = 0\n",
    "    domain_acc = 0\n",
    "    total_label_loss = 0\n",
    "    label_acc = 0\n",
    "    #print(batches)\n",
    "    for i, train_data in enumerate(batches):\n",
    "        print(train_data)\n",
    "        (source_x_v, source_x_a,source_x_l, source_y_a), \\\n",
    "        (target_x_v, target_x_a, target_x_l) = train_data\n",
    "\n",
    "        source_x_v = source_x_v.to(device)\n",
    "        source_x_a = source_x_a.to(device)\n",
    "        source_x_l = source_x_l.to(device)\n",
    "        source_y_a = source_y_a.to(device)\n",
    "\n",
    "        print(source_x_v)\n",
    "        print(source_x_a)\n",
    "        \n",
    "        print(source_x_l)\n",
    "        print(source_y_a)\n",
    "        target_x_v = target_x_v.to(device)\n",
    "        target_x_a = target_x_a.to(device)\n",
    "        target_x_l = target_x_l.to(device)\n",
    "\n",
    "        source_encoded_x = model.encoder(source_x_v, source_x_a, source_x_l)\n",
    "        target_encoded_x = model.encoder(target_x_v, target_x_a, target_x_l)\n",
    "\n",
    "        encoded_x = torch.cat([source_encoded_x, target_encoded_x])\n",
    "        encoded_x = encoded_x.to(device)\n",
    "\n",
    "        domain_y = torch.cat([\ttorch.ones(source_encoded_x.shape[0], dtype=torch.int64),\n",
    "                            torch.zeros(target_encoded_x.shape[0], dtype=torch.int64)]).to(device)\n",
    "        label_y = source_y_a.to(device)\n",
    "\n",
    "        domain_preds = discriminator(encoded_x).squeeze()\n",
    "        label_preds = model.recognizer(source_encoded_x)\n",
    "\n",
    "        domain_loss = criterion(domain_preds, domain_y)\n",
    "        label_loss = criterion(label_preds, label_y)\n",
    "        loss = domain_loss + label_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_domain_loss += domain_loss.item()\n",
    "        total_label_loss += label_loss.item()\n",
    "\n",
    "        domain_preds = domain_preds.argmax(dim=1, keepdim=True)\n",
    "        domain_acc += domain_preds.eq(domain_y.view_as(domain_preds)).sum().item() / len(domain_preds)\n",
    "\n",
    "        label_preds = label_preds.argmax(dim=1, keepdim=True)\n",
    "        label_acc += label_preds.eq(label_y.view_as(label_preds)).sum().item() / len(label_preds)\n",
    "\n",
    "        if opt.verbose and i > 0 and i % int(n_batches / 10) == 0:\n",
    "            print('.', flush=True, end='')\n",
    "\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "\n",
    "    domain_loss = total_domain_loss / n_batches\n",
    "    domain_acc = domain_acc / n_batches\n",
    "    label_loss = total_label_loss / n_batches\n",
    "    label_acc = label_acc / n_batches\n",
    "\n",
    "    return domain_loss, domain_acc, label_loss, label_acc\n",
    "\n",
    "def test(test_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        groundtruth = []\n",
    "        prediction = []\n",
    "        \n",
    "\n",
    "        for i, test_data in enumerate(test_loader):\n",
    "            #print(test_data)\n",
    "            acoustic_features =[{\"input_values\": test_data[\"input_values\"][0][0].to(device)}]\n",
    "            #labels = torch.tensor(float(test_data[\"labels\"]))\n",
    "            labels = torch.tensor([test_data[\"labels\"]])\n",
    "            #acoustic_features, a_labels = test_data\n",
    "\n",
    "            \n",
    "                    \n",
    "            #acoustic_features = [{\"input_values\":acoustic_features[0][0]}]\n",
    "\n",
    "            #acoustic_features = acoustic_features.cuda()\n",
    "            labels = labels.to(device)\n",
    "            #print(labels)\n",
    "            \n",
    "            # final_features = Wav2Vec2Processor.pad(\n",
    "            #     acoustic_features,\n",
    "            #     padding=True,\n",
    "            #     return_tensors=\"pt\",\n",
    "            # )\n",
    "            logits = model(test_data['input_values'].squeeze(1).to(device)).logits \n",
    "            \n",
    "            predictions = torch.argmax(logits, dim=-1).detach().cpu()\n",
    "            \n",
    "            # predictions = torch.tensor(float(predictions)).unsqueeze(0)\n",
    "            # print(predictions)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            groundtruth.append(labels.tolist())\n",
    "            predictions = predictions.argmax(dim=-1, keepdim=True)\n",
    "            prediction.append(predictions.view_as(labels).tolist())\n",
    "\n",
    "        test_loss = running_loss / len(test_loader)\n",
    "\n",
    "        groundtruth = list(itertools.chain.from_iterable(groundtruth))\n",
    "        prediction = list(itertools.chain.from_iterable(prediction))\n",
    "\n",
    "        test_acc = accuracy_score(prediction, groundtruth)\n",
    "        test_uar = recall_score(prediction, groundtruth, average='macro')\n",
    "\n",
    "        return test_loss, test_acc, test_uar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4929fd8-e89d-4c64-a6a0-37475c5db7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "batch_size = 8\n",
    "epochs_num =  10\n",
    "learning_rate = 1e-3\n",
    "half_batch = batch_size // 2\n",
    "batch_size = half_batch\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "\n",
    "# Dataloaders\n",
    "test_loader = NSCTest\n",
    "\n",
    "# Model, optimizer and loss function\n",
    "# checkpoint = torch.load(os.path.join(\t'checkpoints/bl', opt.source_domain, 'model.pth.tar'),\n",
    "#                     map_location=device)\n",
    "\n",
    "emotion_recognizer = DANN.class_classifier\n",
    "# emotion_recognizer.load_state_dict(checkpoint['emotion_recognizer'])\n",
    "for param in emotion_recognizer.parameters():\n",
    "    param.requires_grad = True\n",
    "emotion_recognizer.to(device)\n",
    "\n",
    "discriminator = DANN.domain_classifier\n",
    "discriminator.apply(init_weights)\n",
    "for param in discriminator.parameters():\n",
    "    param.requires_grad = True\n",
    "discriminator.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\tlist(emotion_recognizer.parameters())\n",
    "                +list(discriminator.parameters()),\n",
    "                lr=learning_rate)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc = 0.\n",
    "best_acc_std = 0.\n",
    "\n",
    "best_uar = 0.\n",
    "best_uar_std = 0.\n",
    "\n",
    "# Train and validate\n",
    "for epoch in tqdm(range(epochs_num)):\n",
    "    print('epoch: {}/{}'.format(epoch + 1, epochs_num))\n",
    "\n",
    "    batch_iterator, n_batches = get_batch_iterator()\n",
    "    \n",
    "    #print(list(batch_iterator))\n",
    "\n",
    "    domain_loss, domain_acc, train_loss, train_acc = train(list(batch_iterator), n_batches,\n",
    "                              emotion_recognizer, discriminator,\n",
    "                              optimizer, criterion, device)\n",
    "    test_loss, test_acc, test_uar = test(NSCTest, emotion_recognizer, criterion, device)\n",
    "\n",
    "#     acc_list = []\n",
    "#     uar_list = []\n",
    "\n",
    "#     _, acc, uar = test(test_loader, emotion_recognizer, criterion, device)\n",
    "#     acc_list.append(acc)\n",
    "#     uar_list.append(uar)\n",
    "\n",
    "#     acc_list = np.array(acc_list)\n",
    "#     uar_list = np.array(uar_list)\n",
    "\n",
    "#     acc_std = np.std(acc_list)\n",
    "#     uar_std = np.std(uar_list)\n",
    "\n",
    "\n",
    "    print(\t'domain_loss: {0:.5f}'.format(domain_loss),\n",
    "        'domain_acc: {0:.3f}'.format(domain_acc),\n",
    "        'train_loss: {0:.5f}'.format(train_loss),\n",
    "        'train_acc: {0:.3f}'.format(train_acc),\n",
    "        'test_loss: {0:.5f}'.format(test_loss),\n",
    "        'test_acc: {0:.3f}'.format(test_acc),\n",
    "        'test_uar: {0:.3f}'.format(test_uar))\n",
    "\n",
    "    #os.makedirs(os.path.join(opt.logger_path, opt.source_domain), exist_ok=True)\n",
    "\n",
    "    model_file_name = 'checkpoint.pth.tar'\n",
    "    state = {\t'epoch': epoch+1, 'emotion_recognizer': emotion_recognizer.state_dict(),\n",
    "        'discriminator' : discriminator.state_dict()}\n",
    "    torch.save(state, model_file_name)\n",
    "\n",
    "    if test_acc > best_acc and epoch >= 3:\n",
    "        model_file_name = 'model.pth.tar'\n",
    "        torch.save(state, model_file_name)\n",
    "\n",
    "        best_acc = test_acc\n",
    "        #best_acc_std = acc_std\n",
    "\n",
    "    if test_uar > best_uar and epoch >= 3:\n",
    "        best_uar = test_uar\n",
    "        #best_uar_std = uar_std\n",
    "\n",
    "print(best_acc, best_uar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c865ba-8667-4518-8b8d-e7680e709019",
   "metadata": {},
   "source": [
    "## trainer = CTCTrainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset['train'],\n",
    "    eval_dataset=val_dataset['train'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43d1c6-645b-4320-b9c6-8c18e57c9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e48ca-3a9c-4dba-8cc2-9d784524e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4f76e-c7a0-4445-9ce7-b26bc323d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78050c1e-74fe-4b7e-ae09-2e8cc9f1eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17fafad-a5bc-4482-a677-b720cde77edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bac6e4-1a77-445c-b377-7ecc4957339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f77ba-41ad-4e28-aa60-0a2518a97092",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randn(3, 5).softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2483f-38dd-42a1-bf52-cafe9727348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8172c-1a69-4fee-845a-6d2070fe4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ea0df4b-3c04-4c2a-a505-3a3867549992",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = get_batch_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d4156ca5-d212-4b39-9c74-53d0865cf9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b574087-ee63-440f-b8aa-fbf35652e39d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_proj",
   "language": "python",
   "name": "dl_proj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
